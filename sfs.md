## Chow-Liu Trees

## Generative AI Models - Lecture 5

9 th May 2025

Thomas Krak (slides adapted from Gennaro Gala) Uncertainty in Artificial Intelligence

<!-- image -->

## Idea: structure learning as discrete optimization

- Â· Let X be a set of RVs and D = { x n } N n =1 be i.i.d. data
- Â· Let [ G ] be some family of DAGs over X
- Â· Define a suitable score S G D ( , )
- Â· Find G âˆ— = arg max Gâˆˆ G [ ] S G D ( , )

- Â· [ G ] is the set of all directed trees [ T ] over X
- Â· Directed tree : Every RV has at most one parent
- Â· Score S G D ( , ) = max Î˜ L G ( , Î˜ , D ), where
- Â· Î˜ are all BN parameters for G (for categorical CPDs)
- Â· L G ( , Î˜ , D ) is the log-likelihood
- Â· Thus, tree G is 'better' than G ' if the log-likelihood of G is higher than the log-likelihood of G ' (when equipping them with their ML parameters):

<!-- image -->

- Â· Remarkable: Poly-time Algorithm!

## Algorithm 3 VE\_PRI(N, Q, J)

## input:

N:

Q

- variables in network N

Bayesian network

- J: ordering of network variables not in Q

output: the marginal Pr(Q) prior

## main:

- 1:
- 3:
- 4:
- 5: replace all factors fk in $ by factor fi
- 6: end for
- 7: return IIfes f
- = 1 to length of order I do

## Approximating Discrece Probability Distributions with Dependence Trees

AND C. N. LIU, MEMBER, IEEE

<!-- image -->

## Learning Chow-Liu Trees

Inference in Tree-shaped BNs

Ancestral Sampling

## Learning Chow-Liu Trees

(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:

<!-- formula-not-decoded -->

(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:

<!-- formula-not-decoded -->

In other words, it is the expectation of the logarithmic difference between the distributions p and q , where the expectation is taken using the distribution p , i.e.:

<!-- formula-not-decoded -->

(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:

<!-- formula-not-decoded -->

In other words, it is the expectation of the logarithmic difference between the distributions p and q , where the expectation is taken using the distribution p , i.e.:

<!-- formula-not-decoded -->

Ì¸

Note that, in general, KL ( p || q ) = KL ( q || p ) and that KL ( p || q ) = 0 iff p = q .

(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:

<!-- formula-not-decoded -->

(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:

<!-- formula-not-decoded -->

- Â· The mutual information of two RVs is a measure of the mutual dependence

(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:

<!-- formula-not-decoded -->

- Â· The mutual information of two RVs is a measure of the mutual dependence
- Â· Note that, MI is measured in nats (natural unit of information) when the natural logarithm is used.

| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |
|------------------|---------|---------|-------------|
| y = 0            | 0 1 .   | 0 3 .   | 0 4 .       |
| y = 1            | 0 2 .   | 0 4 .   | 0 6 .       |
| p X ( X )        | 0 3 .   | 0 7 .   |             |

| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |
|------------------|---------|---------|-------------|
| y = 0            | 0 1 .   | 0 3 .   | 0 4 .       |
| y = 1            | 0 2 .   | 0 4 .   | 0 6 .       |
| p X ( X )        | 0 3 .   | 0 7 .   |             |

<!-- formula-not-decoded -->

| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |
|------------------|---------|---------|-------------|
| y = 0            | 0 08 .  | 0 32 .  | 0 4 .       |
| y = 1            | 0 12 .  | 0 48 .  | 0 6 .       |
| p X ( X )        | 0 2 .   | 0 8 .   |             |

| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |
|------------------|---------|---------|-------------|
| y = 0            | 0 08 .  | 0 32 .  | 0 4 .       |
| y = 1            | 0 12 .  | 0 48 .  | 0 6 .       |
| p X ( X )        | 0 2 .   | 0 8 .   |             |

<!-- formula-not-decoded -->

## Bayesian Networks

A Bayesian Network (BN) over RVs X = ( X i ) d i =1 is a pair ( G P , ), where:

- Â· G is a DAG which has RVs X as nodes;
- Â· P is a collection of distributions p X ( i | pa ( X i ));

## and where:

<!-- formula-not-decoded -->

## Bayesian Networks

A Bayesian Network (BN) over RVs X = ( X i ) d i =1 is a pair ( G P , ), where:

- Â· G is a DAG which has RVs X as nodes;
- P is a collection of distributions p X ( i | pa ( X i ));

<!-- formula-not-decoded -->

<!-- image -->

p ( X ) = p X ( 1 | X 2 , X 5 ) p X ( 2 ) p X ( 3 | X 1 ) p X ( 4 | X 1 ) p X ( 5 )

- Â· and where:

## Tree-shaped Bayesian Networks

A tree-shaped BN over RVs X = ( X i ) d i =1 is a pair ( T , P ), where:

- Â· T is a directed tree which has RVs X as nodes;
- Â· P is a collection of distributions p X ( i | X Ï„ ( ) i ), where
- X Ï„ ( ) i is the parent of X i in T ;

and where:

<!-- formula-not-decoded -->

If X i is the root of T then Ï„ ( ) = 0 and i p X ( i | X 0 ) = p X ( i ).

## Tree-shaped Bayesian Networks

A tree-shaped BN over RVs X = ( X i ) d i =1 is a pair ( T , P ), where:

- Â· T is a directed tree which has RVs X as nodes;
- Â· P is a collection of distributions p X ( i | X Ï„ ( ) i ), where X Ï„ ( ) i is the parent of X i in T ;

and where:

<!-- formula-not-decoded -->

<!-- image -->

If X i is the root of T then Ï„ ( ) = 0 and i p X ( i | X 0 ) = p X ( i ).

<!-- formula-not-decoded -->

- Â· We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p âˆ— ( X )

- Â· We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p âˆ— ( X )
- Â· We want to learn the 'best' tree-shaped BN ( T , P ) from D

- Â· We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p âˆ— ( X )
- Â· We want to learn the 'best' tree-shaped BN ( T , P ) from D
- Â· In other words, we want to find the best tree-based approximation p ( X ) = d âˆ i =1 p âˆ— ( X X i | Ï„ ( ) i ) of p âˆ— ( X )

- Â· Cayley's formula is a result in graph theory named after Arthur Cayley. It states that for every positive integer d , the number of trees on d labeled vertices is d d -2
- Â· The number of possible trees for any moderate value of d is so enormous as to exlude any approach of exhaustive search

## How many possible trees?

- Â· Cayley's formula is a result in graph theory named after Arthur Cayley. It states that for every positive integer d , the number of trees on d labeled vertices is d d -2
- Â· The number of possible trees for any moderate value of d is so enormous as to exlude any approach of exhaustive search

<!-- image -->

We want to find T s.t. its induced probability distribution p ( X ) = âˆ d i =1 p âˆ— ( X X i | Ï„ ( ) i ) is as close as possible to the true unknown distribution p âˆ— ( X ).

We want to find T s.t. its induced probability distribution p ( X ) = âˆ d i =1 p âˆ— ( X X i | Ï„ ( ) i ) is as close as possible to the true unknown distribution p âˆ— ( X ).

<!-- formula-not-decoded -->

We want to find T s.t. its induced probability distribution p ( X ) = âˆ d i =1 p âˆ— ( X X i | Ï„ ( ) i ) is as close as possible to the true unknown distribution p âˆ— ( X ).

<!-- formula-not-decoded -->

We want to find T s.t. its induced probability distribution p ( X ) = âˆ d i =1 p âˆ— ( X X i | Ï„ ( ) i ) is as close as possible to the true unknown distribution p âˆ— ( X ).

<!-- formula-not-decoded -->

We want to find T s.t. its induced probability distribution p ( X ) = âˆ d i =1 p âˆ— ( X X i | Ï„ ( ) i ) is as close as possible to the true unknown distribution p âˆ— ( X ).

<!-- formula-not-decoded -->

Since E âˆ¼ x p âˆ— [log p âˆ— ( x )] is independent of T , only the second quantity matters.

## Chow-Liu Algorithm - The Proof \ 2

<!-- formula-not-decoded -->

## Chow-Liu Algorithm - The Proof \ 2

<!-- formula-not-decoded -->

## Chow-Liu Algorithm - The Proof \ 2

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

Therefore, minimising KL ( p âˆ— || p ) is equivalent to maximizing âˆ‘ d i =1 MI( X i , X Ï„ ( ) i ) over all possible trees.

## Maximum spanning tree

Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .

<!-- formula-not-decoded -->

## Maximum spanning tree

Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .

<!-- formula-not-decoded -->

<!-- image -->

ğ‘‹

.6

4

5

.6

3

ğ‘‹

4

.

67

ğ‘‹

1

ğ‘‹

.7

2

3

ğ‘‹

2

## Maximum spanning tree

Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .

<!-- formula-not-decoded -->

<!-- image -->

## Maximum spanning tree

Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .

<!-- image -->

- Â· A maximum spanning tree is a subset of the edges of a connected undirected graph that connects all the vertices together, without any cycles and with the maximum possible total edge weight
- Â· Kruskal's algorithm finds the maximum spanning tree in polynomial time

## Orienting the Tree

Recall: minimising KL ( p âˆ— || p ) is equivalent to maximizing âˆ‘ d i =1 MI( X i , X Ï„ ( ) i ).

Mutual information is symmetric: MI( X i , X Ï„ ( ) i ) = MI( X Ï„ ( ) i , X i )

So direction of the arcs does not impact KL ( p || p

âˆ— )!

To orient the undirected maximum spanning tree:

- Â· Choose any node as the root;
- Â· Orient all edges to point away from the root

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

<!-- formula-not-decoded -->

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## Parameter estimation

## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

where Î± &gt; 0 is a smoothing parameter for the Laplace's correction. Usually Î± = 0 01. .

## Chow-Liu Algorithm

| Algorithm 1 Learn-CLT ( D , Î± )                                                       |
|---------------------------------------------------------------------------------------|
| Input: A set of samples D = { x ( n ) } N n =1 over RVs X and a smoothing parameter Î± |
| Output: A CLT ( T , P ) over RVs X 1: MI â† estimateMI( D , Î± )                        |
| 2: T â† maximumSpanningTree( MI )                                                      |
| 3: T â† directedTree( T )                                                              |
| 4: P â† estimatePMFs( T , D , Î± )                                                      |
| 5: return âŸ¨T , PâŸ©                                                                     |

<!-- image -->

## Chow-Liu Trees:

- Â· Maximum-likelihood fit to given data over space of tree-shaped BNs
- Â· Based on maximum-spanning tree for pairwise mutual information
- Â· Runs in polynomial time using e.g. Kruskal's or Prim's algorithm

## Inference in Tree-shaped BNs

## Suppose we have a tree-shaped BN ( T , P )

- Â· For instance, a CLT

We now want to perform inference with it:

- Â· Marginal inference
- Â· Most Probably Explanation

How can we do this efficiently?

## Consider a CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

Consider a CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).

<!-- image -->

p x ( 1 = 1 , x 2 = 0 , x 3 = 1 , x 4 = 1 , x 5 = 0) = 0 7 . Â· 0 6 . Â· 0 6 . Â· 0 2 . Â· 0 4 = 0 02016 . .

## Exhaustive inference: p x ( 2 = 0 , x 5 = 1) = 0 258 .

|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |
|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|
|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |
|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |
|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |
|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |
|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |
|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |
|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |
|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |
|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |
|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |
|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |
|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |
|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |
|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |
|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |
|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |

So, we need something smarter

## Variable Elimination

## input:

Bayesian network

variables in network N

- I: ordering of network variables not in Q

output: the marginal Pr(Q) prior

## main:

- 1: S&lt; CPTs of network N
- 2: for i = 1 to length of order T do
- 3: to $ and mentions variable I (i ) belongs
- 5: replace all factors fk in $ by factor fi
- 6: end for
- 7: return f [lfes \_

## Variable Elimination in Trees

## When using reverse topological order on a tree-structured BN:

- Â· The order width 1 w = 1, so VE will run in O ( n exp ( w )) = O ( n ) time!
- Â· Algorithm can be elegantly restructured as message passing method

- Â· Every non-root node sends messages to its parent
- Â· Every node can send a message if and only if it has received messages from all its children
- Â· We denote by Âµ Xi â†’ X Ï„ ( ) i ; x the message sent from X i to its parent X Ï„ ( ) i when X Ï„ ( ) i = x

<!-- image -->

## Marginal Inference: The Sum-Product Algorithm

- Â· Let ( T , P ) be a tree-shaped BN over X = { X i } d i =1 and X r the root of T
- Â· We want to compute p (Ë†) where Ë† x x âˆˆ Ë† X and Ë† X âŠ† X

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

## Marginal Inference: How to compute p x ( 2 = 0 , x 5 = 1) ?

- Â· We use X 3 â‰» X 4 â‰» X 2 â‰» X 5 â‰» X 1 as reversed topological order

<!-- image -->

p x ( 2 = 0 , x 5 = 1) = p x ( 1 = 0) Â· Âµ X 2 â†’ X 1;0 Â· Âµ X 5 â†’ X 1;0 + p x ( 1 = 1) Â· Âµ X 2 â†’ X 1;1 Â· Âµ X 5 â†’ X 1;1 = 0 3 . Â· (0 2 . Â· 0 1) + 0 7 . . Â· (0 6 . Â· 0 6) = 0 258 . .

## Marginal Inference: How to compute p x ( 2 = 0 , x 3 = 1 , x 4 = 1) ?

- Â· We use X 3 â‰» X 4 â‰» X 2 â‰» X 5 â‰» X 1 as reversed topological order

<!-- image -->

p x ( 2 = 0 , x 3 = 1 , x 4 = 1) = p x ( 1 = 0) Â· Âµ X 2 â†’ X 1;0 Â· Âµ X 5 â†’ X 1;0 + p x ( 1 = 1) Â· Âµ X 2 â†’ X 1;1 Â· Âµ X 5 â†’ X 1;1 = 0 3 . Â· (0 2 . Â· 0 153) + 0 7 . . Â· (0 6 . Â· 0 318) = 0 14274 . .

- Â· The Most Probable Explanation (MPE) task computes the most probable state of variables that do not have evidence
- Â· The difference between standard inference and MPE inference is that instead of summing values, the maximum is used

## Application: data imputation, e.g. inpainting

<!-- image -->

<!-- image -->

## Exhaustive inference: What is the most probable state?

|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |
|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|
|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |
|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |
|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |
|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |
|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |
|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |
|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |
|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |
|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |
|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |
|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |
|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |
|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |
|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |
|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |
|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |

|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |
|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|
|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |
|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |
|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |
|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |
|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |
|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |
|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |
|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |
|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |
|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |
|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |
|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |
|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |
|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |
|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |
|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |

## MPE Inference: The Max-Product Algorithm

- Â· Let ( T , P ) be a tree-shaped BN over X = { X i } d i =1 and X r the root of T
- Â· Ë† x âˆˆ Ë† , X Ë† X âŠ† X and Z = X \ Ë† X
- Â· We want to compute max z âˆˆ Z p (Ë† x , z ) âˆ max z âˆˆ Z p ( z x | Ë†)

<!-- formula-not-decoded -->

<!-- formula-not-decoded -->

<!-- image -->

max x âˆˆ X p ( x ) = max[( p x ( 1 = 0) Â· Âµ X 2 â†’ X 1;0 Â· Âµ X 5 â†’ X 1;0 ) , ( p x ( 1 = 1) Â· Âµ X 2 â†’ X 1;1 Â· Âµ X 5 â†’ X 1;1 )]

<!-- formula-not-decoded -->

x 1 = 1 = â‡’ x 2 = 0 and x 5 = 1 x 5 = 1 = â‡’ x 3 = 1 and x 4 = 0

<!-- image -->

```
Ëœ Âµ X 3 â†’ X 5 ;0 = max[ 4 . ,. 6] = .6 [ [1] ] Ëœ Âµ X 4 â†’ X 5 ;0 = max[ 8 . ,. 2] = .8 [ [0] ] Ëœ Âµ X 2 â†’ X 1 ;0 = 8 . [ [1] ] Ëœ Âµ X 2 â†’ X 1 ;1 = 4 . [ [1] ] Ëœ Âµ X 5 â†’ X 1 ;0 = ( 9 . Â· .6 Â· .8 ) = . 432 [ [0] ] Ëœ Âµ X 5 â†’ X 1 ;1 = ( 4 . Â· .6 Â· .8 ) = . 192 [ [0] ]
```

max z âˆˆ Z p (Ë† x , z ) = max[( p x ( 1 = 0) Â· Âµ X 2 â†’ X 1;0 Â· Âµ X 5 â†’ X 1;0 ) , ( p x ( 1 = 1) Â· Âµ X 2 â†’ X 1;1 Â· Âµ X 5 â†’ X 1;1 )]

<!-- formula-not-decoded -->

x 1 = 0 and x 2 = 1 and x 5 = 0 x 5 = 0 = â‡’ x 3 = 1 and x 4 = 0

## Inference in Tree-Shaped BNs

<!-- image -->

## Efficient inference with VE using reverse topological order

- Â· This has order width w = 1, so VE then has complexity O ( n )

Algorithm can be restructured as message passing method

- Â· Sum-Product algorithm for marginal inference
- Â· Max-Product algorithm for MPE

## Ancestral Sampling

## Ancestral Sampling

Method to draw i.i.d. samples x âˆ¼ p ( X ), where p ( X ) is (encoded by) a BN ( G P , )

- Â· Requires method to sample x âˆ¼ p X ( | pa ( X )) for each X
- Â· E.g. inverse-transform sampling

## Ancestral Sampling

Method to draw i.i.d. samples x âˆ¼ p ( X ), where p ( X ) is (encoded by) a BN ( G P , )

- Â· Requires method to sample x âˆ¼ p X ( | pa ( X )) for each X
- Â· E.g. inverse-transform sampling

Simply use topological order Ï€ of G . For each i = 1 , . . . , | Ï€ | ,

- Â· Let Ï i = pa ( X Ï€ ( ) i )
- Â· Sample x Ï€ ( ) i âˆ¼ p X ( Ï€ ( ) i | X Ï i = x Ï i ), where x Ï i are values already sampled

Then x âˆ¼ p ( X )

- Â· Let X âˆ¼ B ( p ) a Bernoulli RV with probability p . To sample from X we generate a random number Ïµ âˆˆ [0 , 1] if Ïµ â‰¤ p then x = 1 else x = 0.
- Â· We use X 1 â‰º X 2 â‰º X 5 â‰º X 3 â‰º X 4 as topological order.

<!-- image -->

- 1. rand([0, 1]) = 0.8 â†’ x 1 = 0
- 2. rand([0, 1]) = 0.3 â†’ x 2 = 1
- 3. rand([0, 1]) = 0.5 â†’ x 5 = 0
- 4. rand([0, 1]) = 0.1 â†’ x 3 = 1
- 5. rand([0, 1]) = 0.6 â†’ x 4 = 0

## Summary and Outlook

## Today's lecture

- Â· Chow-Liu Trees
- Â· Inference in tree-shaped BNs
- Â· Ancestral sampling

## Next lecture

- Â· Markov networks
- Â· missing data