{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document L05_AY2425.pdf\n",
      "INFO:docling.document_converter:Finished converting document L05_AY2425.pdf in 14.75 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Chow-Liu Trees\n",
      "\n",
      "## Generative AI Models - Lecture 5\n",
      "\n",
      "9 th May 2025\n",
      "\n",
      "Thomas Krak (slides adapted from Gennaro Gala) Uncertainty in Artificial Intelligence\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Idea: structure learning as discrete optimization\n",
      "\n",
      "- · Let X be a set of RVs and D = { x n } N n =1 be i.i.d. data\n",
      "- · Let [ G ] be some family of DAGs over X\n",
      "- · Define a suitable score S G D ( , )\n",
      "- · Find G ∗ = arg max G∈ G [ ] S G D ( , )\n",
      "\n",
      "- · [ G ] is the set of all directed trees [ T ] over X\n",
      "- · Directed tree : Every RV has at most one parent\n",
      "- · Score S G D ( , ) = max Θ L G ( , Θ , D ), where\n",
      "- · Θ are all BN parameters for G (for categorical CPDs)\n",
      "- · L G ( , Θ , D ) is the log-likelihood\n",
      "- · Thus, tree G is 'better' than G ' if the log-likelihood of G is higher than the log-likelihood of G ' (when equipping them with their ML parameters):\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- · Remarkable: Poly-time Algorithm!\n",
      "\n",
      "## Algorithm 3 VE\\_PRI(N, Q, J)\n",
      "\n",
      "## input:\n",
      "\n",
      "N:\n",
      "\n",
      "Q\n",
      "\n",
      "- variables in network N\n",
      "\n",
      "Bayesian network\n",
      "\n",
      "- J: ordering of network variables not in Q\n",
      "\n",
      "output: the marginal Pr(Q) prior\n",
      "\n",
      "## main:\n",
      "\n",
      "- 1:\n",
      "- 3:\n",
      "- 4:\n",
      "- 5: replace all factors fk in $ by factor fi\n",
      "- 6: end for\n",
      "- 7: return IIfes f\n",
      "- = 1 to length of order I do\n",
      "\n",
      "## Approximating Discrece Probability Distributions with Dependence Trees\n",
      "\n",
      "AND C. N. LIU, MEMBER, IEEE\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Learning Chow-Liu Trees\n",
      "\n",
      "Inference in Tree-shaped BNs\n",
      "\n",
      "Ancestral Sampling\n",
      "\n",
      "## Learning Chow-Liu Trees\n",
      "\n",
      "(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In other words, it is the expectation of the logarithmic difference between the distributions p and q , where the expectation is taken using the distribution p , i.e.:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "(Kullback-Leibler divergence) Let p and q be probability distributions over the same state space X . The Kullback-Leibler divergence between p and q is defined as:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "In other words, it is the expectation of the logarithmic difference between the distributions p and q , where the expectation is taken using the distribution p , i.e.:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "̸\n",
      "\n",
      "Note that, in general, KL ( p || q ) = KL ( q || p ) and that KL ( p || q ) = 0 iff p = q .\n",
      "\n",
      "(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "- · The mutual information of two RVs is a measure of the mutual dependence\n",
      "\n",
      "(Mutual Information) Given two jointly discrete RVs X and Y with joint distribution p XY and marginal distributions p X and p Y , the mutual information MI( X Y ; ) between X and Y is:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "- · The mutual information of two RVs is a measure of the mutual dependence\n",
      "- · Note that, MI is measured in nats (natural unit of information) when the natural logarithm is used.\n",
      "\n",
      "| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |\n",
      "|------------------|---------|---------|-------------|\n",
      "| y = 0            | 0 1 .   | 0 3 .   | 0 4 .       |\n",
      "| y = 1            | 0 2 .   | 0 4 .   | 0 6 .       |\n",
      "| p X ( X )        | 0 3 .   | 0 7 .   |             |\n",
      "\n",
      "| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |\n",
      "|------------------|---------|---------|-------------|\n",
      "| y = 0            | 0 1 .   | 0 3 .   | 0 4 .       |\n",
      "| y = 1            | 0 2 .   | 0 4 .   | 0 6 .       |\n",
      "| p X ( X )        | 0 3 .   | 0 7 .   |             |\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |\n",
      "|------------------|---------|---------|-------------|\n",
      "| y = 0            | 0 08 .  | 0 32 .  | 0 4 .       |\n",
      "| y = 1            | 0 12 .  | 0 48 .  | 0 6 .       |\n",
      "| p X ( X )        | 0 2 .   | 0 8 .   |             |\n",
      "\n",
      "| p XY ( X Y , )   | x = 0   | x = 1   | p Y ( Y )   |\n",
      "|------------------|---------|---------|-------------|\n",
      "| y = 0            | 0 08 .  | 0 32 .  | 0 4 .       |\n",
      "| y = 1            | 0 12 .  | 0 48 .  | 0 6 .       |\n",
      "| p X ( X )        | 0 2 .   | 0 8 .   |             |\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Bayesian Networks\n",
      "\n",
      "A Bayesian Network (BN) over RVs X = ( X i ) d i =1 is a pair ( G P , ), where:\n",
      "\n",
      "- · G is a DAG which has RVs X as nodes;\n",
      "- · P is a collection of distributions p X ( i | pa ( X i ));\n",
      "\n",
      "## and where:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Bayesian Networks\n",
      "\n",
      "A Bayesian Network (BN) over RVs X = ( X i ) d i =1 is a pair ( G P , ), where:\n",
      "\n",
      "- · G is a DAG which has RVs X as nodes;\n",
      "- P is a collection of distributions p X ( i | pa ( X i ));\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "p ( X ) = p X ( 1 | X 2 , X 5 ) p X ( 2 ) p X ( 3 | X 1 ) p X ( 4 | X 1 ) p X ( 5 )\n",
      "\n",
      "- · and where:\n",
      "\n",
      "## Tree-shaped Bayesian Networks\n",
      "\n",
      "A tree-shaped BN over RVs X = ( X i ) d i =1 is a pair ( T , P ), where:\n",
      "\n",
      "- · T is a directed tree which has RVs X as nodes;\n",
      "- · P is a collection of distributions p X ( i | X τ ( ) i ), where\n",
      "- X τ ( ) i is the parent of X i in T ;\n",
      "\n",
      "and where:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "If X i is the root of T then τ ( ) = 0 and i p X ( i | X 0 ) = p X ( i ).\n",
      "\n",
      "## Tree-shaped Bayesian Networks\n",
      "\n",
      "A tree-shaped BN over RVs X = ( X i ) d i =1 is a pair ( T , P ), where:\n",
      "\n",
      "- · T is a directed tree which has RVs X as nodes;\n",
      "- · P is a collection of distributions p X ( i | X τ ( ) i ), where X τ ( ) i is the parent of X i in T ;\n",
      "\n",
      "and where:\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "If X i is the root of T then τ ( ) = 0 and i p X ( i | X 0 ) = p X ( i ).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "- · We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p ∗ ( X )\n",
      "\n",
      "- · We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p ∗ ( X )\n",
      "- · We want to learn the 'best' tree-shaped BN ( T , P ) from D\n",
      "\n",
      "- · We are given a dataset D = { x ( n ) } N n =1 drawn from an unknown distribution p ∗ ( X )\n",
      "- · We want to learn the 'best' tree-shaped BN ( T , P ) from D\n",
      "- · In other words, we want to find the best tree-based approximation p ( X ) = d ∏ i =1 p ∗ ( X X i | τ ( ) i ) of p ∗ ( X )\n",
      "\n",
      "- · Cayley's formula is a result in graph theory named after Arthur Cayley. It states that for every positive integer d , the number of trees on d labeled vertices is d d -2\n",
      "- · The number of possible trees for any moderate value of d is so enormous as to exlude any approach of exhaustive search\n",
      "\n",
      "## How many possible trees?\n",
      "\n",
      "- · Cayley's formula is a result in graph theory named after Arthur Cayley. It states that for every positive integer d , the number of trees on d labeled vertices is d d -2\n",
      "- · The number of possible trees for any moderate value of d is so enormous as to exlude any approach of exhaustive search\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "We want to find T s.t. its induced probability distribution p ( X ) = ∏ d i =1 p ∗ ( X X i | τ ( ) i ) is as close as possible to the true unknown distribution p ∗ ( X ).\n",
      "\n",
      "We want to find T s.t. its induced probability distribution p ( X ) = ∏ d i =1 p ∗ ( X X i | τ ( ) i ) is as close as possible to the true unknown distribution p ∗ ( X ).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We want to find T s.t. its induced probability distribution p ( X ) = ∏ d i =1 p ∗ ( X X i | τ ( ) i ) is as close as possible to the true unknown distribution p ∗ ( X ).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We want to find T s.t. its induced probability distribution p ( X ) = ∏ d i =1 p ∗ ( X X i | τ ( ) i ) is as close as possible to the true unknown distribution p ∗ ( X ).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "We want to find T s.t. its induced probability distribution p ( X ) = ∏ d i =1 p ∗ ( X X i | τ ( ) i ) is as close as possible to the true unknown distribution p ∗ ( X ).\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Since E ∼ x p ∗ [log p ∗ ( x )] is independent of T , only the second quantity matters.\n",
      "\n",
      "## Chow-Liu Algorithm - The Proof \\ 2\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Chow-Liu Algorithm - The Proof \\ 2\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Chow-Liu Algorithm - The Proof \\ 2\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Therefore, minimising KL ( p ∗ || p ) is equivalent to maximizing ∑ d i =1 MI( X i , X τ ( ) i ) over all possible trees.\n",
      "\n",
      "## Maximum spanning tree\n",
      "\n",
      "Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Maximum spanning tree\n",
      "\n",
      "Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "𝑋\n",
      "\n",
      ".6\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      ".6\n",
      "\n",
      "3\n",
      "\n",
      "𝑋\n",
      "\n",
      "4\n",
      "\n",
      ".\n",
      "\n",
      "67\n",
      "\n",
      "𝑋\n",
      "\n",
      "1\n",
      "\n",
      "𝑋\n",
      "\n",
      ".7\n",
      "\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "𝑋\n",
      "\n",
      "2\n",
      "\n",
      "## Maximum spanning tree\n",
      "\n",
      "Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Maximum spanning tree\n",
      "\n",
      "Let MI be the Mutual Information matrix of X = ( X i ) 5 i =1 .\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- · A maximum spanning tree is a subset of the edges of a connected undirected graph that connects all the vertices together, without any cycles and with the maximum possible total edge weight\n",
      "- · Kruskal's algorithm finds the maximum spanning tree in polynomial time\n",
      "\n",
      "## Orienting the Tree\n",
      "\n",
      "Recall: minimising KL ( p ∗ || p ) is equivalent to maximizing ∑ d i =1 MI( X i , X τ ( ) i ).\n",
      "\n",
      "Mutual information is symmetric: MI( X i , X τ ( ) i ) = MI( X τ ( ) i , X i )\n",
      "\n",
      "So direction of the arcs does not impact KL ( p || p\n",
      "\n",
      "∗ )!\n",
      "\n",
      "To orient the undirected maximum spanning tree:\n",
      "\n",
      "- · Choose any node as the root;\n",
      "- · Orient all edges to point away from the root\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Parameter estimation\n",
      "\n",
      "## A CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "where α &gt; 0 is a smoothing parameter for the Laplace's correction. Usually α = 0 01. .\n",
      "\n",
      "## Chow-Liu Algorithm\n",
      "\n",
      "| Algorithm 1 Learn-CLT ( D , α )                                                       |\n",
      "|---------------------------------------------------------------------------------------|\n",
      "| Input: A set of samples D = { x ( n ) } N n =1 over RVs X and a smoothing parameter α |\n",
      "| Output: A CLT ( T , P ) over RVs X 1: MI ← estimateMI( D , α )                        |\n",
      "| 2: T ← maximumSpanningTree( MI )                                                      |\n",
      "| 3: T ← directedTree( T )                                                              |\n",
      "| 4: P ← estimatePMFs( T , D , α )                                                      |\n",
      "| 5: return ⟨T , P⟩                                                                     |\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Chow-Liu Trees:\n",
      "\n",
      "- · Maximum-likelihood fit to given data over space of tree-shaped BNs\n",
      "- · Based on maximum-spanning tree for pairwise mutual information\n",
      "- · Runs in polynomial time using e.g. Kruskal's or Prim's algorithm\n",
      "\n",
      "## Inference in Tree-shaped BNs\n",
      "\n",
      "## Suppose we have a tree-shaped BN ( T , P )\n",
      "\n",
      "- · For instance, a CLT\n",
      "\n",
      "We now want to perform inference with it:\n",
      "\n",
      "- · Marginal inference\n",
      "- · Most Probably Explanation\n",
      "\n",
      "How can we do this efficiently?\n",
      "\n",
      "## Consider a CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Consider a CLT ( T , P ) encoding p ( X ) = p X ( 1 ) p X ( 2 | X 1 ) p X ( 3 | X 5 ) p X ( 4 | X 5 ) p X ( 5 | X 1 ).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "p x ( 1 = 1 , x 2 = 0 , x 3 = 1 , x 4 = 1 , x 5 = 0) = 0 7 . · 0 6 . · 0 6 . · 0 2 . · 0 4 = 0 02016 . .\n",
      "\n",
      "## Exhaustive inference: p x ( 2 = 0 , x 5 = 1) = 0 258 .\n",
      "\n",
      "|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |\n",
      "|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|\n",
      "|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |\n",
      "|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |\n",
      "|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |\n",
      "|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |\n",
      "|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |\n",
      "|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |\n",
      "|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |\n",
      "|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |\n",
      "|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |\n",
      "|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |\n",
      "|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |\n",
      "|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |\n",
      "|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |\n",
      "|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |\n",
      "|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |\n",
      "|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |\n",
      "\n",
      "So, we need something smarter\n",
      "\n",
      "## Variable Elimination\n",
      "\n",
      "## input:\n",
      "\n",
      "Bayesian network\n",
      "\n",
      "variables in network N\n",
      "\n",
      "- I: ordering of network variables not in Q\n",
      "\n",
      "output: the marginal Pr(Q) prior\n",
      "\n",
      "## main:\n",
      "\n",
      "- 1: S&lt; CPTs of network N\n",
      "- 2: for i = 1 to length of order T do\n",
      "- 3: to $ and mentions variable I (i ) belongs\n",
      "- 5: replace all factors fk in $ by factor fi\n",
      "- 6: end for\n",
      "- 7: return f [lfes \\_\n",
      "\n",
      "## Variable Elimination in Trees\n",
      "\n",
      "## When using reverse topological order on a tree-structured BN:\n",
      "\n",
      "- · The order width 1 w = 1, so VE will run in O ( n exp ( w )) = O ( n ) time!\n",
      "- · Algorithm can be elegantly restructured as message passing method\n",
      "\n",
      "- · Every non-root node sends messages to its parent\n",
      "- · Every node can send a message if and only if it has received messages from all its children\n",
      "- · We denote by µ Xi → X τ ( ) i ; x the message sent from X i to its parent X τ ( ) i when X τ ( ) i = x\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Marginal Inference: The Sum-Product Algorithm\n",
      "\n",
      "- · Let ( T , P ) be a tree-shaped BN over X = { X i } d i =1 and X r the root of T\n",
      "- · We want to compute p (ˆ) where ˆ x x ∈ ˆ X and ˆ X ⊆ X\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "## Marginal Inference: How to compute p x ( 2 = 0 , x 5 = 1) ?\n",
      "\n",
      "- · We use X 3 ≻ X 4 ≻ X 2 ≻ X 5 ≻ X 1 as reversed topological order\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "p x ( 2 = 0 , x 5 = 1) = p x ( 1 = 0) · µ X 2 → X 1;0 · µ X 5 → X 1;0 + p x ( 1 = 1) · µ X 2 → X 1;1 · µ X 5 → X 1;1 = 0 3 . · (0 2 . · 0 1) + 0 7 . . · (0 6 . · 0 6) = 0 258 . .\n",
      "\n",
      "## Marginal Inference: How to compute p x ( 2 = 0 , x 3 = 1 , x 4 = 1) ?\n",
      "\n",
      "- · We use X 3 ≻ X 4 ≻ X 2 ≻ X 5 ≻ X 1 as reversed topological order\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "p x ( 2 = 0 , x 3 = 1 , x 4 = 1) = p x ( 1 = 0) · µ X 2 → X 1;0 · µ X 5 → X 1;0 + p x ( 1 = 1) · µ X 2 → X 1;1 · µ X 5 → X 1;1 = 0 3 . · (0 2 . · 0 153) + 0 7 . . · (0 6 . · 0 318) = 0 14274 . .\n",
      "\n",
      "- · The Most Probable Explanation (MPE) task computes the most probable state of variables that do not have evidence\n",
      "- · The difference between standard inference and MPE inference is that instead of summing values, the maximum is used\n",
      "\n",
      "## Application: data imputation, e.g. inpainting\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Exhaustive inference: What is the most probable state?\n",
      "\n",
      "|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |\n",
      "|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|\n",
      "|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |\n",
      "|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |\n",
      "|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |\n",
      "|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |\n",
      "|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |\n",
      "|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |\n",
      "|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |\n",
      "|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |\n",
      "|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |\n",
      "|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |\n",
      "|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |\n",
      "|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |\n",
      "|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |\n",
      "|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |\n",
      "|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |\n",
      "|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |\n",
      "\n",
      "|   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |   x 1 |   x 2 |   x 3 |   x 4 |   x 5 |   p ( x ) |\n",
      "|-------|-------|-------|-------|-------|-----------|-------|-------|-------|-------|-------|-----------|\n",
      "|     0 |     0 |     0 |     0 |     0 |   0.01728 |     1 |     0 |     0 |     0 |     0 |   0.05376 |\n",
      "|     0 |     0 |     0 |     0 |     1 |   0.0003  |     1 |     0 |     0 |     0 |     1 |   0.0126  |\n",
      "|     0 |     0 |     0 |     1 |     0 |   0.00432 |     1 |     0 |     0 |     1 |     0 |   0.01344 |\n",
      "|     0 |     0 |     0 |     1 |     1 |   0.0003  |     1 |     0 |     0 |     1 |     1 |   0.0126  |\n",
      "|     0 |     0 |     1 |     0 |     0 |   0.02592 |     1 |     0 |     1 |     0 |     0 |   0.08064 |\n",
      "|     0 |     0 |     1 |     0 |     1 |   0.0027  |     1 |     0 |     1 |     0 |     1 |   0.1134  |\n",
      "|     0 |     0 |     1 |     1 |     0 |   0.00648 |     1 |     0 |     1 |     1 |     0 |   0.02016 |\n",
      "|     0 |     0 |     1 |     1 |     1 |   0.0027  |     1 |     0 |     1 |     1 |     1 |   0.1134  |\n",
      "|     0 |     1 |     0 |     0 |     0 |   0.06912 |     1 |     1 |     0 |     0 |     0 |   0.03584 |\n",
      "|     0 |     1 |     0 |     0 |     1 |   0.0012  |     1 |     1 |     0 |     0 |     1 |   0.0084  |\n",
      "|     0 |     1 |     0 |     1 |     0 |   0.01728 |     1 |     1 |     0 |     1 |     0 |   0.00896 |\n",
      "|     0 |     1 |     0 |     1 |     1 |   0.0012  |     1 |     1 |     0 |     1 |     1 |   0.0084  |\n",
      "|     0 |     1 |     1 |     0 |     0 |   0.10368 |     1 |     1 |     1 |     0 |     0 |   0.05376 |\n",
      "|     0 |     1 |     1 |     0 |     1 |   0.0108  |     1 |     1 |     1 |     0 |     1 |   0.0756  |\n",
      "|     0 |     1 |     1 |     1 |     0 |   0.02592 |     1 |     1 |     1 |     1 |     0 |   0.01344 |\n",
      "|     0 |     1 |     1 |     1 |     1 |   0.0108  |     1 |     1 |     1 |     1 |     1 |   0.0756  |\n",
      "\n",
      "## MPE Inference: The Max-Product Algorithm\n",
      "\n",
      "- · Let ( T , P ) be a tree-shaped BN over X = { X i } d i =1 and X r the root of T\n",
      "- · ˆ x ∈ ˆ , X ˆ X ⊆ X and Z = X \\ ˆ X\n",
      "- · We want to compute max z ∈ Z p (ˆ x , z ) ∝ max z ∈ Z p ( z x | ˆ)\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "max x ∈ X p ( x ) = max[( p x ( 1 = 0) · µ X 2 → X 1;0 · µ X 5 → X 1;0 ) , ( p x ( 1 = 1) · µ X 2 → X 1;1 · µ X 5 → X 1;1 )]\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "x 1 = 1 = ⇒ x 2 = 0 and x 5 = 1 x 5 = 1 = ⇒ x 3 = 1 and x 4 = 0\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "```\n",
      "˜ µ X 3 → X 5 ;0 = max[ 4 . ,. 6] = .6 [ [1] ] ˜ µ X 4 → X 5 ;0 = max[ 8 . ,. 2] = .8 [ [0] ] ˜ µ X 2 → X 1 ;0 = 8 . [ [1] ] ˜ µ X 2 → X 1 ;1 = 4 . [ [1] ] ˜ µ X 5 → X 1 ;0 = ( 9 . · .6 · .8 ) = . 432 [ [0] ] ˜ µ X 5 → X 1 ;1 = ( 4 . · .6 · .8 ) = . 192 [ [0] ]\n",
      "```\n",
      "\n",
      "max z ∈ Z p (ˆ x , z ) = max[( p x ( 1 = 0) · µ X 2 → X 1;0 · µ X 5 → X 1;0 ) , ( p x ( 1 = 1) · µ X 2 → X 1;1 · µ X 5 → X 1;1 )]\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "x 1 = 0 and x 2 = 1 and x 5 = 0 x 5 = 0 = ⇒ x 3 = 1 and x 4 = 0\n",
      "\n",
      "## Inference in Tree-Shaped BNs\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Efficient inference with VE using reverse topological order\n",
      "\n",
      "- · This has order width w = 1, so VE then has complexity O ( n )\n",
      "\n",
      "Algorithm can be restructured as message passing method\n",
      "\n",
      "- · Sum-Product algorithm for marginal inference\n",
      "- · Max-Product algorithm for MPE\n",
      "\n",
      "## Ancestral Sampling\n",
      "\n",
      "## Ancestral Sampling\n",
      "\n",
      "Method to draw i.i.d. samples x ∼ p ( X ), where p ( X ) is (encoded by) a BN ( G P , )\n",
      "\n",
      "- · Requires method to sample x ∼ p X ( | pa ( X )) for each X\n",
      "- · E.g. inverse-transform sampling\n",
      "\n",
      "## Ancestral Sampling\n",
      "\n",
      "Method to draw i.i.d. samples x ∼ p ( X ), where p ( X ) is (encoded by) a BN ( G P , )\n",
      "\n",
      "- · Requires method to sample x ∼ p X ( | pa ( X )) for each X\n",
      "- · E.g. inverse-transform sampling\n",
      "\n",
      "Simply use topological order π of G . For each i = 1 , . . . , | π | ,\n",
      "\n",
      "- · Let ρ i = pa ( X π ( ) i )\n",
      "- · Sample x π ( ) i ∼ p X ( π ( ) i | X ρ i = x ρ i ), where x ρ i are values already sampled\n",
      "\n",
      "Then x ∼ p ( X )\n",
      "\n",
      "- · Let X ∼ B ( p ) a Bernoulli RV with probability p . To sample from X we generate a random number ϵ ∈ [0 , 1] if ϵ ≤ p then x = 1 else x = 0.\n",
      "- · We use X 1 ≺ X 2 ≺ X 5 ≺ X 3 ≺ X 4 as topological order.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "- 1. rand([0, 1]) = 0.8 → x 1 = 0\n",
      "- 2. rand([0, 1]) = 0.3 → x 2 = 1\n",
      "- 3. rand([0, 1]) = 0.5 → x 5 = 0\n",
      "- 4. rand([0, 1]) = 0.1 → x 3 = 1\n",
      "- 5. rand([0, 1]) = 0.6 → x 4 = 0\n",
      "\n",
      "## Summary and Outlook\n",
      "\n",
      "## Today's lecture\n",
      "\n",
      "- · Chow-Liu Trees\n",
      "- · Inference in tree-shaped BNs\n",
      "- · Ancestral sampling\n",
      "\n",
      "## Next lecture\n",
      "\n",
      "- · Markov networks\n",
      "- · missing data\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "source = \"/home/akshay/projects/generativeAI/L05_AY2425.pdf\"  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(source,)\n",
    "print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "result.document.save_as_markdown(filename=Path(\"sfs.md\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections.abc import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from docling_core.types.doc import DocItemLabel, DoclingDocument, NodeItem, TextItem\n",
    "\n",
    "\n",
    "from docling.datamodel.base_models import InputFormat, ItemAndImageEnrichmentElement\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.models.base_model import BaseItemAndImageEnrichmentModel\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "\n",
    "\n",
    "class ExampleFormulaUnderstandingPipelineOptions(PdfPipelineOptions):\n",
    "    do_formula_understanding: bool = True\n",
    "\n",
    "# A new enrichment model using both the document element and its image as input\n",
    "class ExampleFormulaUnderstandingEnrichmentModel(BaseItemAndImageEnrichmentModel):\n",
    "    images_scale = 2.6\n",
    "\n",
    "    def __init__(self, enabled: bool):\n",
    "        self.enabled = enabled\n",
    "\n",
    "    def is_processable(self, doc: DoclingDocument, element: NodeItem) -> bool:\n",
    "        return (\n",
    "            self.enabled\n",
    "            and isinstance(element, TextItem)\n",
    "            and element.label == DocItemLabel.FORMULA\n",
    "        )\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        doc: DoclingDocument,\n",
    "        element_batch: Iterable[ItemAndImageEnrichmentElement],\n",
    "    ) -> Iterable[NodeItem]:\n",
    "        if not self.enabled:\n",
    "            return\n",
    "\n",
    "        for enrich_element in element_batch:\n",
    "            enrich_element.image.show()\n",
    "\n",
    "            yield enrich_element.item\n",
    "\n",
    "\n",
    "# How the pipeline can be extended.\n",
    "class ExampleFormulaUnderstandingPipeline(StandardPdfPipeline):\n",
    "    def __init__(self, pipeline_options: ExampleFormulaUnderstandingPipelineOptions):\n",
    "        super().__init__(pipeline_options)\n",
    "        self.pipeline_options: ExampleFormulaUnderstandingPipelineOptions\n",
    "\n",
    "        self.enrichment_pipe = [\n",
    "            ExampleFormulaUnderstandingEnrichmentModel(\n",
    "                enabled=self.pipeline_options.do_formula_understanding\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        if self.pipeline_options.do_formula_understanding:\n",
    "            self.keep_backend = True\n",
    "\n",
    "    @classmethod\n",
    "    def get_default_options(cls) -> ExampleFormulaUnderstandingPipelineOptions:\n",
    "        return ExampleFormulaUnderstandingPipelineOptions()\n",
    "    \n",
    "\n",
    "# Example main. In the final version, we simply have to set do_formula_understanding to true.\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    input_doc_path = Path(\"/home/akshay/projects/generativeAI/L05_AY2425.pdf\")\n",
    "\n",
    "    pipeline_options = ExampleFormulaUnderstandingPipelineOptions()\n",
    "    pipeline_options.do_formula_understanding = True\n",
    "\n",
    "    doc_converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(\n",
    "                pipeline_cls=ExampleFormulaUnderstandingPipeline,\n",
    "                pipeline_options=pipeline_options,\n",
    "            )\n",
    "        }\n",
    "    )\n",
    "    doc_converter.convert(input_doc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.utils.accelerator_utils:Accelerator device: 'cuda:0'\n",
      "INFO:docling.pipeline.base_pipeline:Processing document L05_AY2425.pdf\n",
      "INFO:docling.document_converter:Finished converting document L05_AY2425.pdf in 16.13 sec.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starlight",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
